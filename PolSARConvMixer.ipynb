{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLkYMq3FgZtEWMnTtTHh7Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aj1365/PolSARConvMixer/blob/main/PolSARConvMixer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbUlqkQjEtdC"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.layers import Conv2D, Dense, Reshape, BatchNormalization\n",
        "from keras.layers import Dropout, Input\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\n",
        "from operator import truediv\n",
        "from plotly.offline import init_notebook_mode\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as sio\n",
        "import os\n",
        "import spectral\n",
        "from keras import layers\n",
        "import tensorflow as tf\n",
        "init_notebook_mode(connected=True)\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loadData(name):\n",
        "\n",
        "    data_path = os.path.join(os.getcwd(),'C:/PolSAR/Data/')\n",
        "\n",
        "    if name == 'Flevoland':\n",
        "\n",
        "        data = sio.loadmat(os.path.join(data_path, 'Flevoland_T3RF.mat'))['T3RF']\n",
        "        labels = sio.loadmat(os.path.join(data_path, 'Flevoland_15cls.mat'))['label']\n",
        "\n",
        "    if name == 'SanFrancisco':\n",
        "\n",
        "        data = sio.loadmat(os.path.join(data_path, 'SanFrancisco_T3RF.mat'))['T3RF']\n",
        "        labels = sio.loadmat(os.path.join(data_path, 'SanFrancisco_gt.mat'))['SanFrancisco_gt']\n",
        "\n",
        "\n",
        "    return data, labels"
      ],
      "metadata": {
        "id": "fE5pRqFpFLxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## GLOBAL VARIABLES\n",
        "dataset = 'SanFrancisco'\n",
        "test_ratio = 0.9\n",
        "windowSize = 12"
      ],
      "metadata": {
        "id": "t6yCTzjXFNly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def splitTrainTestSet(X, y, testRatio, randomState=345):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testRatio, random_state=randomState,\n",
        "                                                        stratify=y)\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "RU9zq9gpFN0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def applyPCA(X, numComponents=75):\n",
        "    newX = np.reshape(X, (-1, X.shape[2]))\n",
        "    pca = PCA(n_components=numComponents, whiten=True)\n",
        "    newX = pca.fit_transform(newX)\n",
        "    newX = np.reshape(newX, (X.shape[0],X.shape[1], numComponents))\n",
        "    return newX, pca"
      ],
      "metadata": {
        "id": "kJASCRfsFSJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def padWithZeros(X, margin=2):\n",
        "    newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n",
        "    x_offset = margin\n",
        "    y_offset = margin\n",
        "    newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n",
        "    return newX"
      ],
      "metadata": {
        "id": "SMQS5g-bFSMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createImageCubes(X, y, windowSize=8, removeZeroLabels = True):\n",
        "    margin = int((windowSize) / 2)\n",
        "    zeroPaddedX = padWithZeros(X, margin=margin)\n",
        "    # split patches\n",
        "    patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]))\n",
        "    patchesLabels = np.zeros((X.shape[0] * X.shape[1]))\n",
        "    patchIndex = 0\n",
        "    for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
        "        for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
        "            patch = zeroPaddedX[r - margin:r + margin , c - margin:c + margin ]\n",
        "            patchesData[patchIndex, :, :, :] = patch\n",
        "            patchesLabels[patchIndex] = y[r-margin, c-margin]\n",
        "            patchIndex = patchIndex + 1\n",
        "    if removeZeroLabels:\n",
        "        patchesData = patchesData[patchesLabels>0,:,:,:]\n",
        "        patchesLabels = patchesLabels[patchesLabels>0]\n",
        "        patchesLabels -= 1\n",
        "    return patchesData, patchesLabels"
      ],
      "metadata": {
        "id": "mnfz-AjMFSPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X , Y = loadData(dataset)\n",
        "X=(X-np.min(X))/(np.max(X)-np.min(X))"
      ],
      "metadata": {
        "id": "iCMGIfp1FSSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X1, Y1 = createImageCubes(X, Y, windowSize=windowSize)\n",
        "X1.shape, Y1.shape"
      ],
      "metadata": {
        "id": "7d8L7J-KFSVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xtrain2, Xtest2, ytrain2, ytest2 = splitTrainTestSet(X1, Y1, test_ratio)\n",
        "Xtrain, Xtest, ytrain, ytest = splitTrainTestSet(Xtrain2, ytrain2, 0.5)\n",
        "\n",
        "Xtrain.shape, Xtest.shape, ytrain.shape, ytest.shape"
      ],
      "metadata": {
        "id": "VcfT79VUFchK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del Xtrain2, Xtest2, Xtrain, Xtest, X1, Y1"
      ],
      "metadata": {
        "id": "Iv8p6JWuFckd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "image_size=12\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(image_size, image_size),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2\n",
        "        ),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "# Compute the mean and the variance of the training data for normalization.\n",
        "data_augmentation.layers[0].adapt(Xtrain)"
      ],
      "metadata": {
        "id": "Z6amBIfdFcnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    ####################################################################### Create CNN model\n",
        "#kSize=3\n",
        "nBands=12\n",
        "n_outputs=5"
      ],
      "metadata": {
        "id": "pGScDWgBFjHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def activation_block(x):\n",
        "    x = layers.Activation(\"gelu\")(x)\n",
        "    return layers.BatchNormalization()(x)\n",
        "\n",
        "def conv_stem(x, filters: int, patch_size: int):\n",
        "    x = layers.Conv2D(filters, kernel_size=patch_size, strides=patch_size)(x)\n",
        "    return activation_block(x)\n",
        "\n",
        "def conv_mixer_block(x, filters: int, kernel_size: int):\n",
        "    # Depthwise convolution.\n",
        "    x0 = x\n",
        "\n",
        "    pos_emb1 = layers.DepthwiseConv2D(kernel_size=3, padding=\"same\")(x)\n",
        "    pos_emb2 = layers.DepthwiseConv2D(kernel_size=5, padding=\"same\")(x)\n",
        "\n",
        "    x = keras.layers.Add()([x0, pos_emb1, pos_emb2])\n",
        "\n",
        "    # Pointwise convolution.\n",
        "    x = layers.Conv2D(filters, kernel_size=1)(x)\n",
        "    x = activation_block(x)\n",
        "    x = layers.Add()([x, x0])  # Residual 2.\n",
        "\n",
        "    return x\n",
        "\n",
        "def get_PolSAR_conv_mixer(\n",
        "    image_size=12, filters=256, depth=4, kernel_size=3, patch_size=2, num_classes=5\n",
        "):\n",
        "\n",
        "    inputs = keras.Input((image_size, image_size, 12))\n",
        "    augmented = data_augmentation(inputs)\n",
        "    x=augmented\n",
        "    # Extract patch embeddings.\n",
        "    x = conv_stem(x, filters, patch_size)\n",
        "\n",
        "    # ConvMixer blocks.\n",
        "    for _ in range(depth):\n",
        "        x = conv_mixer_block(x, filters, kernel_size)\n",
        "\n",
        "    # Classification block.\n",
        "    x = layers.GlobalAvgPool2D()(x)\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    return keras.Model(inputs, outputs)\n",
        "  model=get_PolSAR_conv_mixer()"
      ],
      "metadata": {
        "id": "L5t--gCoFjK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_decay = 0.0001\n",
        "batch_size = 256\n",
        "dropout_rate = 0.4\n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "oWQGQT9UFjNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\n",
        "from operator import truediv\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def AA_andEachClassAccuracy(confusion_matrix):\n",
        "    counter = confusion_matrix.shape[0]\n",
        "    list_diag = np.diag(confusion_matrix)\n",
        "    list_raw_sum = np.sum(confusion_matrix, axis=1)\n",
        "    each_acc = np.nan_to_num(truediv(list_diag, list_raw_sum))\n",
        "    average_acc = np.mean(each_acc)\n",
        "    return average_acc\n",
        "\n",
        "\n",
        "# Define per-fold score containers\n",
        "loss_function = sparse_categorical_crossentropy\n",
        "no_classes = 5\n",
        "no_epochs = 5\n",
        "optimizer = Adam()\n",
        "verbosity = 1\n",
        "num_folds = 3\n",
        "aa_per_fold = []\n",
        "oa_per_fold = []\n",
        "ki_per_fold = []\n",
        "\n",
        "loss_function = sparse_categorical_crossentropy\n",
        "# Merge inputs and targets\n",
        "inputs = np.concatenate((Xtrain, Xtest), axis=0)\n",
        "targets = np.concatenate((ytrain, ytest), axis=0)\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "\n",
        "\n",
        "  # Define the model architecture\n",
        "    model=get_conv_mixer()\n",
        "  # Compile the model\n",
        "    optimizer = keras.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "\n",
        "  # Generate a print\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "    history = model.fit(inputs[train], targets[train],\n",
        "              batch_size=batch_size,\n",
        "              epochs=5)\n",
        "\n",
        "\n",
        "\n",
        "  # Generate generalization metrics\n",
        "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "\n",
        "    Y_pred = model.predict(inputs[test])\n",
        "    y_pred = np.argmax(Y_pred, axis=1)\n",
        "    confusion = confusion_matrix(targets[test], y_pred)\n",
        "    oa = accuracy_score(targets[test], y_pred)\n",
        "\n",
        "    oa_per_fold.append(oa * 100)\n",
        "    aa = AA_andEachClassAccuracy(confusion)\n",
        "    aa_per_fold.append(aa * 100)\n",
        "    kappa = cohen_kappa_score(targets[test], y_pred)\n",
        "    ki_per_fold.append(kappa * 100)\n",
        "\n",
        "\n",
        "  # Increase fold number\n",
        "    fold_no = fold_no + 1\n"
      ],
      "metadata": {
        "id": "kdS7s896FjQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'> OA: {np.mean(oa_per_fold)} (+- {np.std(oa_per_fold)})')\n",
        "print(f'> AA: {np.mean(aa_per_fold)} (+- {np.std(aa_per_fold)})')\n",
        "print(f'> KI: {np.mean(ki_per_fold)} (+- {np.std(ki_per_fold)})')"
      ],
      "metadata": {
        "id": "9rJ2X4e0Fcqa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}